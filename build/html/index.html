

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to Spark over Ceph’s documentation! &mdash; Spark over Ceph  文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'./',
              VERSION:'',
              LANGUAGE:'zh_CN',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/translations.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> Spark over Ceph
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to Spark over Ceph&#8217;s documentation!</a></li>
<li><a class="reference internal" href="#spark-on-ceph-why">1. Spark On Ceph： Why?</a></li>
<li><a class="reference internal" href="#spark-on-ceph-how">2. Spark On Ceph： How?</a></li>
<li><a class="reference internal" href="#docker">3. Docker的安装</a></li>
<li><a class="reference internal" href="#ceph">4. Ceph的安装及配置</a><ul>
<li><a class="reference internal" href="#a">a. 创建配置文件夹并授权</a></li>
<li><a class="reference internal" href="#b-mon-mgr-osd-mgr-mds">b. 创建mon, mgr, osd, mgr, mds安装脚本并执行</a></li>
<li><a class="reference internal" href="#c-ceph">c. ceph测试</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hadoop">5. Hadoop的安装</a></li>
<li><a class="reference internal" href="#spark">6. Spark的安装</a></li>
<li><a class="reference internal" href="#sparkceph">7. 在Spark中使用Ceph</a><ul>
<li><a class="reference internal" href="#a-spark">a. spark集群测试</a></li>
<li><a class="reference internal" href="#b-s3">b. 使用S3</a></li>
<li><a class="reference internal" href="#c-hadoop-cephfs">c. 使用hadoop-cephfs</a><ul>
<li><a class="reference internal" href="#spark-hdfs">1). Spark + HDFS</a></li>
<li><a class="reference internal" href="#hadoop-cephfs">2). Hadoop + CephFS</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Spark over Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to Spark over Ceph&#8217;s documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="welcome-to-spark-over-ceph-s-documentation">
<h1>Welcome to Spark over Ceph&#8217;s documentation!<a class="headerlink" href="#welcome-to-spark-over-ceph-s-documentation" title="永久链接至标题">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">实验是在Centos 7虚拟机的Docker中进行，Ceph, Hadoop, Spark均为容器应用,真机部署时可能需要更多配置，但流程类似。</p>
</div>
</div>
<div class="section" id="spark-on-ceph-why">
<h1>1. Spark On Ceph： Why?<a class="headerlink" href="#spark-on-ceph-why" title="永久链接至标题">¶</a></h1>
<p>Spark和Hadoop都是主流的大数据处理平台，
两者都可以进行分布式计算任务，而且Spark既可以作为Hadoop的模块，任务通过Yarn提交在Hadoop集群中运行，也可以作为一个独立的解决方案直接在Spark集群中处理任务。Spark没有自己的文件管理功能，
这意味着在处理大数据任务时必须依赖某种存储解决方案（也可以直接从本地文件系统读取），例如HDFS，HBase，NFS，S3等。由于Spark在内存中处理一切数据，相对于Hadoop来讲，性能更强，处理速度更快，
成本更低，关于两者更多的信息可以参考 <a class="reference external" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a> 和 <a class="reference external" href="https://spark.apache.org/">https://spark.apache.org/</a> 。</p>
<p>将Ceph与Spark结合的初衷是为了解决数据孤岛问题。通常，每个单独的分析群集通常都有自己的非共享HDFS数据存储，为了提供跨孤岛的相同数据集的访问，数据平台团队经常在HDFS孤岛之间复制数据集，
试图使它们保持一致和最新，这样做的结果就是公司最终维护了许多独立的固定分析集群，成本高昂。AWS生态系统通过Hadoop S3A文件系统客户端提供了对象存储方案，Spark和Hadoop都可以直接面向共享S3对象存储中的数据进行查询或提交任务，Ceph是排名第一的开源私有云对象存储平台，提供与S3兼容的对象存储，自然成为首选方案。详情请参考
<a class="reference external" href="https://www.redhat.com/en/blog/why-spark-ceph-part-1-3">https://www.redhat.com/en/blog/why-spark-ceph-part-1-3</a>，<a class="reference external" href="https://www.redhat.com/en/blog/why-spark-ceph-part-2-3">https://www.redhat.com/en/blog/why-spark-ceph-part-2-3</a>，<a class="reference external" href="https://www.redhat.com/en/blog/why-spark-ceph-part-3-3">https://www.redhat.com/en/blog/why-spark-ceph-part-3-3</a></p>
</div>
<div class="section" id="spark-on-ceph-how">
<h1>2. Spark On Ceph： How?<a class="headerlink" href="#spark-on-ceph-how" title="永久链接至标题">¶</a></h1>
<p>实际上Spark并不能与Ceph分布式存储系统直接结合，因此只能通过某种桥梁将Spark与Ceph结合起来。参考<a class="reference external" href="https://indico.cern.ch/event/524549/contributions/2185930/attachments/1290231/1921189/2016.06.13_-_Spark_on_Ceph.pdf">这里</a> ，在这份材料里，提到了三种解决方案，分别是：<a class="reference external" href="https://www.slideshare.net/zhouyuan/hadoop-over-rgw">RGWFS</a>， <a class="reference external" href="https://nwat.xyz/blog/2015/07/12/hadoop-on-ceph-diving-in/">CephFS-Hadoop</a>， 以及<a class="reference external" href="http://docs.ceph.com/docs/master/radosgw/s3/">S3 gateway endpoint</a>。实际上由于RGWFS方案复杂，没有更多的细节，甚至项目地址已经不存在了。CephFS-Hadoop技术方案虽可行，Spark可以通过HDFS来对CephFS进行数据读写，但Ceph官方也没有随版本迭代而进行维护（<a class="reference external" href="http://docs.ceph.com/docs/master/cephfs/hadoop/">过时的文档</a>），而且数据读取速率低下。虽然本次实验会尝试实现后两种方案，但实际上第三种方案或许会成为首选。</p>
</div>
<div class="section" id="docker">
<h1>3. Docker的安装<a class="headerlink" href="#docker" title="永久链接至标题">¶</a></h1>
<p>实验都在docker环境中进行，首先需要安装docker环境。在CentOS 7中的docker的安装不在此处说明，请参考<a class="reference external" href="http://www.runoob.com/docker/centos-docker-install.html">菜鸟教程</a>。实验需要使用的镜像可以通过下面的命令获得。</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>docker pull ceph/deamon:latest-mimic
docker pull bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
docker pull bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
docker pull bde2020/spark-master:2.4.0-hadoop2.7
docker pull bde2020/spark-master:2.4.0-hadoop2.7
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">值得说明的是，除了ceph为必须项之外，根据具体实验需求获取镜像，比如只需要做spark访问s3就不需要安装hadoop集群。需要注意安装的顺序。</p>
</div>
</div>
<div class="section" id="ceph">
<h1>4. Ceph的安装及配置<a class="headerlink" href="#ceph" title="永久链接至标题">¶</a></h1>
<blockquote>
<div>本节参考<a class="reference external" href="https://jermine.vdo.pub/ceph/%E5%9F%BA%E4%BA%8Edocker%E9%83%A8%E7%BD%B2ceph%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9Fmimic13.2.4%E7%89%88%E6%9C%AC/">基于Docker部署ceph分布式文件系统</a>并且这里部署的是单机版。在开始之前需要给虚拟机挂载一块新磁盘作为ceph的osd。即dev/sdb &gt; osd。虚拟机ip为192.168.184.164</div></blockquote>
<div class="section" id="a">
<h2>a. 创建配置文件夹并授权<a class="headerlink" href="#a" title="永久链接至标题">¶</a></h2>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="nb">cd</span> ~
mkdir ceph
mkdir ceph/<span class="o">{</span>admin,etc,lib,logs<span class="o">}</span>
chown -R <span class="m">167</span>:167 ~/ceph/
</pre></div>
</div>
</div>
<div class="section" id="b-mon-mgr-osd-mgr-mds">
<h2>b. 创建mon, mgr, osd, mgr, mds安装脚本并执行<a class="headerlink" href="#b-mon-mgr-osd-mgr-mds" title="永久链接至标题">¶</a></h2>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># pwd: ~/run_mon.sh</span>
docker run -d --net<span class="o">=</span>host <span class="se">\</span>
    --name<span class="o">=</span>mon <span class="se">\</span>
    --restart<span class="o">=</span>always <span class="se">\</span>
    -v /etc/localtime:/etc/localtime <span class="se">\</span>
    -v ~/ceph/etc:/etc/ceph <span class="se">\</span>
    -v ~/ceph/lib:/var/lib/ceph <span class="se">\</span>
    -v ~/ceph/logs:/var/log/ceph <span class="se">\</span>
    -e <span class="nv">MON_IP</span><span class="o">=</span><span class="m">192</span>.168.184.164 <span class="se">\</span>
    -e <span class="nv">CEPH_PUBLIC_NETWORK</span><span class="o">=</span><span class="m">192</span>.168.184.0/24 <span class="se">\</span>
    ceph/daemon:latest-mimic mon

<span class="c1">#!/bin/bash</span>
<span class="c1"># pwd: ~/run_mgr.sh</span>
docker run -d --net<span class="o">=</span>host <span class="se">\</span>
    --name<span class="o">=</span>mgr <span class="se">\</span>
    --restart<span class="o">=</span>always <span class="se">\</span>
    --privileged<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    -v /etc/localtime:/etc/localtime <span class="se">\</span>
    -v ~/ceph/etc:/etc/ceph <span class="se">\</span>
    -v ~/ceph/lib:/var/lib/ceph <span class="se">\</span>
    -v ~/ceph/logs:/var/log/ceph <span class="se">\</span>
    ceph/daemon:latest-mimic mgr

<span class="c1">#!/bin/bash</span>
<span class="c1"># pwd: ~/run_osd.sh</span>
docker run -d --net<span class="o">=</span>host <span class="se">\</span>
    --name<span class="o">=</span>osd <span class="se">\</span>
    --restart<span class="o">=</span>always <span class="se">\</span>
    --privileged<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    --pid<span class="o">=</span>host <span class="se">\</span>
    -v /etc/localtime:/etc/localtime <span class="se">\</span>
    -v ~/ceph/etc:/etc/ceph <span class="se">\</span>
    -v ~/ceph/lib:/var/lib/ceph <span class="se">\</span>
    -v ~/ceph/logs:/var/log/ceph <span class="se">\</span>
    -v /dev/:/dev/ <span class="se">\</span>
    -e <span class="nv">OSD_DEVICE</span><span class="o">=</span>/dev/sdb <span class="se">\</span>
    ceph/daemon:latest-mimic osd_ceph_disk

<span class="c1">#!/bin/bash</span>
<span class="c1"># pwd: ~/run_rgw.sh</span>
docker run -d <span class="se">\</span>
    --net<span class="o">=</span>host <span class="se">\</span>
    --name<span class="o">=</span>rgw <span class="se">\</span>
    --restart<span class="o">=</span>always <span class="se">\</span>
    --privileged<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    -v /etc/localtime:/etc/localtime <span class="se">\</span>
    -v ~/ceph/etc:/etc/ceph <span class="se">\</span>
    -v ~/ceph/lib:/var/lib/ceph <span class="se">\</span>
    -v ~/ceph/logs:/var/log/ceph <span class="se">\</span>
    ceph/daemon:latest-mimic rgw

<span class="c1">#!/bin/bash</span>
<span class="c1"># pwd: ~/run_mds.sh</span>
<span class="c1"># 如果不结合cephfs-hadoop可以跳过</span>
docker run -d <span class="se">\</span>
    --net<span class="o">=</span>host <span class="se">\</span>
    --name<span class="o">=</span>mds <span class="se">\</span>
    --restart<span class="o">=</span>always <span class="se">\</span>
    --privileged<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    -v /etc/localtime:/etc/localtime <span class="se">\</span>
    -v ~/ceph/etc:/etc/ceph <span class="se">\</span>
    -v ~/ceph/lib/:/var/lib/ceph/ <span class="se">\</span>
    -v ~/ceph/logs/:/var/log/ceph/ <span class="se">\</span>
    -e <span class="nv">CEPHFS_CREATE</span><span class="o">=</span><span class="m">0</span> <span class="se">\</span>
    -e <span class="nv">CEPHFS_METADATA_POOL_PG</span><span class="o">=</span><span class="m">6</span> <span class="se">\</span>
    -e <span class="nv">CEPHFS_DATA_POOL_PG</span><span class="o">=</span><span class="m">6</span> <span class="se">\</span>
    ceph/daemon:latest-mimic  mds
</pre></div>
</div>
</div>
<div class="section" id="c-ceph">
<h2>c. ceph测试<a class="headerlink" href="#c-ceph" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><ol class="arabic simple">
<li>安装好之后，调整~/ceph/etc/ceph.conf配置文件，如下所示。</li>
</ol>
<div class="highlight-ini"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">fsid</span> <span class="o">=</span> <span class="s">cb93f8f8-7712-4dd9-aade-0bc397cc106d</span>
<span class="na">mon initial members</span> <span class="o">=</span> <span class="s">vm_ceph</span>
<span class="na">mon host</span> <span class="o">=</span> <span class="s">192.168.184.164</span>

<span class="na">osd journal size</span> <span class="o">=</span> <span class="s">1024</span>
<span class="na">log file</span> <span class="o">=</span> <span class="s">/dev/null</span>

<span class="na">osd pool default size</span> <span class="o">=</span> <span class="s">1</span>
<span class="na">osd pool default min size</span> <span class="o">=</span> <span class="s">1</span>

<span class="na">osd pool default pg num</span> <span class="o">=</span> <span class="s">6</span>
<span class="na">osd pool default pgp num</span> <span class="o">=</span> <span class="s">6</span>

<span class="na">mon allow pool delete</span> <span class="o">=</span> <span class="s">true</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>创建S3账号，并采用s3cmd测试S3对象存储</li>
</ol>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="c1"># 创建S3账号</span>
docker <span class="nb">exec</span> -it mon radosgw-admin user create --uid<span class="o">=</span><span class="s2">&quot;spark&quot;</span> --display-name<span class="o">=</span><span class="s2">&quot;spark user&quot;</span>

<span class="c1"># 输出结果，记录如下账号信息</span>
<span class="c1"># &quot;keys&quot;: [</span>
<span class="c1"># {</span>
<span class="c1">#    &quot;user&quot;: &quot;spark&quot;,</span>
<span class="c1">#    &quot;access_key&quot;: &quot;GNM6UQ2A5H2JUNQ96APA&quot;,</span>
<span class="c1">#    &quot;secret_key&quot;: &quot;9pDusNV9x1ZDan84HNLYdx6COBrhpr5k1uuimJkR&quot;</span>
<span class="c1"># }</span>
<span class="c1"># ]</span>

<span class="c1"># 安装s3cmd</span>
<span class="nb">cd</span> ~
pip install s3cmd
<span class="c1"># 对~/.s3cfg配置完成后测试S3存储，注意禁用https</span>
<span class="c1"># 创建存储桶</span>
s3cmd mb s3://spark-bucket
<span class="c1"># 上传本地数据文件</span>
s3cmd put Shakespear.txt s3://spark-bucket/Shakespear.txt
<span class="c1"># 获取</span>
s3cmd get s3://spark-bucket/Shakespear.txt
<span class="c1"># download: &#39;s3://spark-bucket/Shakespear.txt&#39; -&gt; &#39;./Shakespear.txt&#39;  [1 of 1]</span>
<span class="c1"># 5465394 of 5465394   100% in    0s    11.41 MB/s  done</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>创建CephFS，可选。</li>
</ol>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="c1">#创建data pool</span>
docker <span class="nb">exec</span> -it mon ceph osd pool create cephfs_data <span class="m">6</span> <span class="m">6</span>

<span class="c1">#创建metadata pool</span>
docker <span class="nb">exec</span> -it mon ceph osd pool create cephfs_metadata <span class="m">6</span> <span class="m">6</span>

<span class="c1">#创建cephfs</span>
docker <span class="nb">exec</span> -it mon ceph fs new cephfs cephfs_metadata cephfs_data

<span class="c1">#查看信息</span>
docker <span class="nb">exec</span> -it mon ceph fs ls
<span class="c1"># name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li>查看Ceph集群状态</li>
</ol>
<div class="highlight-shell"><div class="highlight"><pre><span></span>docker <span class="nb">exec</span> -it mon ceph -s
<span class="c1">#输出</span>
<span class="c1"># cluster:</span>
<span class="c1">#    id:     cb93f8f8-7712-4dd9-aade-0bc397cc106d</span>
<span class="c1">#    health: HEALTH_OK</span>
<span class="c1">#</span>
<span class="c1"># services:</span>
<span class="c1">#    mon: 1 daemons, quorum vm_ceph</span>
<span class="c1">#    mgr: vm_ceph(active)</span>
<span class="c1">#    mds: cephfs-1/1/1 up  {0=vm_ceph=up:active}</span>
<span class="c1">#    osd: 1 osds: 1 up, 1 in</span>
<span class="c1">#    rgw: 1 daemon active</span>
<span class="c1">#</span>
<span class="c1"># data:</span>
<span class="c1">#    pools:   9 pools, 54 pgs</span>
<span class="c1">#    objects: 221  objects, 6.3 MiB</span>
<span class="c1">#    usage:   2.0 GiB used, 17 GiB / 19 GiB avail</span>
<span class="c1">#    pgs:     54 active+clean</span>
</pre></div>
</div>
<p>至此，Ceph单节点安装成功。</p>
</div></blockquote>
</div>
</div>
<div class="section" id="hadoop">
<h1>5. Hadoop的安装<a class="headerlink" href="#hadoop" title="永久链接至标题">¶</a></h1>
<blockquote>
<div><div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">本节可选，如果只想做spark + S3完全可以跳过此节。另外考虑到容器之间的通信方式，这里采用这里采用了自定义网络ceph_spark和external_links的方式。</p>
</div>
<p>1. 这里使用docker-compose编排工具安装hadoop集群。为了方便后期容器间的通信，
这里首先自定义了一个网络ceph_spark:</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>docker network create ceph_spark
docker network ls
<span class="c1"># 输出信息</span>
<span class="c1"># 344f383aab50        bridge                  bridge              local</span>
<span class="c1"># 0e03c96295d5        ceph_spark              bridge              local</span>
<span class="c1"># 7cd69ff99a9f        hadoopcompose_default   bridge              local</span>
<span class="c1"># 8fc29fd140ad        host                    host                local</span>
<span class="c1"># 749630064a6c        none                    null                local</span>
<span class="c1"># 605130cf9434        root_default            bridge              local</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>docker-compose搭建hadoop集群</li>
</ol>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="c1"># 安装docker-compose</span>
yum install docker-compose
docker-compose up -d
</pre></div>
</div>
<p>其中docker-compose.yml的内容为：</p>
<blockquote>
<div><div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">version</span><span class="p p-Indicator">:</span> <span class="s">&quot;2&quot;</span>

<span class="l l-Scalar l-Scalar-Plain">services</span><span class="p p-Indicator">:</span>
<span class="l l-Scalar l-Scalar-Plain">mynamenode</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
    <span class="l l-Scalar l-Scalar-Plain">volumes</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">hadoop_mynamenode:/hadoop/dfs/name</span>
    <span class="l l-Scalar l-Scalar-Plain">environment</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">CLUSTER_NAME=myhadoop</span>
    <span class="l l-Scalar l-Scalar-Plain">env_file</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">./hadoop.env</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">ports</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;9996:50070&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mon</span>

<span class="l l-Scalar l-Scalar-Plain">mydatanode1</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode1</span>
    <span class="l l-Scalar l-Scalar-Plain">depends_on</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
    <span class="l l-Scalar l-Scalar-Plain">volumes</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">hadoop_mydatanode1:/hadoop/dfs/data</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">env_file</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">./hadoop.env</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mon</span>

<span class="l l-Scalar l-Scalar-Plain">mydatanode2</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode2</span>
    <span class="l l-Scalar l-Scalar-Plain">depends_on</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">volumes</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">hadoop_mydatanode2:/hadoop/dfs/data</span>
    <span class="l l-Scalar l-Scalar-Plain">env_file</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">./hadoop.env</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mon</span>

<span class="l l-Scalar l-Scalar-Plain">volumes</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">hadoop_mynamenode</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">hadoop_mydatanode1</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">hadoop_mydatanode2</span><span class="p p-Indicator">:</span>

<span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">external</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</div></blockquote>
<p>同目录下的hadoop.env</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span>CORE_CONF_fs_defaultFS=hdfs://mynamenode:8020
CORE_CONF_hadoop_http_staticuser_user=root
CORE_CONF_hadoop_proxyuser_hue_hosts=*
CORE_CONF_hadoop_proxyuser_hue_groups=*

HDFS_CONF_dfs_webhdfs_enabled=true
HDFS_CONF_dfs_permissions_enabled=false

YARN_CONF_yarn_log___aggregation___enable=true
YARN_CONF_yarn_resourcemanager_recovery_enabled=true
YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
YARN_CONF_yarn_resourcemanager_fs_state___store_uri=/rmstate
YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs
YARN_CONF_yarn_log_server_url=http://historyserver:8188/applicationhistory/logs/
YARN_CONF_yarn_timeline___service_enabled=true
YARN_CONF_yarn_timeline___service_generic___application___history_enabled=true
YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=true
YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
YARN_CONF_yarn_timeline___service_hostname=historyserver
YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032
YARN_CONF_yarn_resourcemanager_scheduler_address=resourcemanager:8030
YARN_CONF_yarn_resourcemanager_resource___tracker_address=resourcemanager:8031
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>进入mynamenode测试hdfs</li>
</ol>
<div class="highlight-shell"><div class="highlight"><pre><span></span><span class="c1">#将测试数据拷入mynamenode内</span>
docker cp ~/Shakespear.txt mynamenode:/root

docker <span class="nb">exec</span> -it mynamenode /bin/bash
<span class="c1"># 进入mynamenode后的命令</span>
<span class="nb">cd</span> /root
hdfs dfs -mkdir /data
hdfs dfs -put Shakespear.txt /data/
</pre></div>
</div>
<p>通过浏览器查看:</p>
<img alt="_images/hdfs_data.png" src="_images/hdfs_data.png" />
</div></blockquote>
</div>
<div class="section" id="spark">
<h1>6. Spark的安装<a class="headerlink" href="#spark" title="永久链接至标题">¶</a></h1>
<blockquote>
<div><p>Spark集群将有一个master节点和两个worker节点组成。和hadoop的安装方式相同，这里依然采用docker-compose，文件内容如下：</p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">version</span><span class="p p-Indicator">:</span> <span class="s">&#39;2&#39;</span>

<span class="l l-Scalar l-Scalar-Plain">myspark-master</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/spark-master:2.4.0-hadoop2.7</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myspark-master</span>
    <span class="l l-Scalar l-Scalar-Plain">hostname</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myspark-master</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">ports</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;4040:4040&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;8042:8042&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;7077:7077&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;8080:8080&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;8088:8088&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">rgw</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode1</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode2</span>
    <span class="l l-Scalar l-Scalar-Plain">restart</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">always</span>
    <span class="l l-Scalar l-Scalar-Plain">environment</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">INIT_DAEMON_STEP=setup_spark</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;constraint:node==myspark-master&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">myspark-worker1</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/spark-worker:2.4.0-hadoop2.7</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myspark-worker1</span>
    <span class="l l-Scalar l-Scalar-Plain">hostname</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myworker1</span>
    <span class="l l-Scalar l-Scalar-Plain">depends_on</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">myspark-master</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">ports</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;8081:8081&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">rgw</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode1</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode2</span>
    <span class="l l-Scalar l-Scalar-Plain">restart</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">always</span>
    <span class="l l-Scalar l-Scalar-Plain">environment</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;SPARK_MASTER=spark://myspark-master:7077&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;constraint:node==myworker1&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">myspark-worker2</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">bde2020/spark-worker:2.4.0-hadoop2.7</span>
    <span class="l l-Scalar l-Scalar-Plain">container_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myspark-worker2</span>
    <span class="l l-Scalar l-Scalar-Plain">hostname</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">myworker2</span>
    <span class="l l-Scalar l-Scalar-Plain">depends_on</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">myspark-master</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span>
    <span class="l l-Scalar l-Scalar-Plain">ports</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;8082:8082&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">external_links</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">rgw</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mynamenode</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode1</span>
     <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">mydatanode2</span>
    <span class="l l-Scalar l-Scalar-Plain">restart</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">always</span>
    <span class="l l-Scalar l-Scalar-Plain">environment</span><span class="p p-Indicator">:</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;SPARK_MASTER=spark://myspark-master:7077&quot;</span>
     <span class="p p-Indicator">-</span> <span class="s">&quot;constraint:node==myworker2&quot;</span>

<span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">ceph_spark</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">external</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>安装好后在浏览器查看Spark集群：</p>
<img alt="_images/spark_s.png" src="_images/spark_s.png" />
</div></blockquote>
</div>
<div class="section" id="sparkceph">
<h1>7. 在Spark中使用Ceph<a class="headerlink" href="#sparkceph" title="永久链接至标题">¶</a></h1>
<p>本节中首先会计算pi值来测试spark集群是否能够正常工作，然后通过统计Shakespeare作品的单词次数程序案例来使用Spark和Ceph。前提工作需要在spark集群各个节点安装pyspark，命令如下：</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>pip install pyspark -i https://pypi.tuna.tsinghua.edu.cn/simple/
</pre></div>
</div>
<div class="section" id="a-spark">
<h2>a. spark集群测试<a class="headerlink" href="#a-spark" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>提交任务:</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>/spark/bin/spark-submit --master spark://172.18.0.2:7077 test_spark_run_pi.py
</pre></div>
</div>
<p>其中test_spark_run_pi.py:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Usage: pi [partitions]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
        <span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PythonPi&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="n">partitions</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">2</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10000000</span> <span class="o">*</span> <span class="n">partitions</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">partitions</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Pi is roughly </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mf">4.0</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p>输出结果如下，表明spark集群能够正常工作</p>
<img alt="_images/spark_rpi.png" src="_images/spark_rpi.png" />
</div></blockquote>
</div>
<div class="section" id="b-s3">
<h2>b. 使用S3<a class="headerlink" href="#b-s3" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>使用S3需要<a class="reference external" href="http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4.1/aws-java-sdk-1.7.4.1.jar">aws-java-sdk.jar</a> 和<a class="reference external" href="https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar">hadoop-aws-2.7.3.jar</a> 。提交任务：</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>/spark/bin/spark-submit --jars aws-java-sdk-1.7.4.jar,hadoop-aws-2.7.3.jar --master spark://172.18.0.2:7077 wordcount_s3.py
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># wordcount_s3.py</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>


<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
        <span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;WordsCount&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="n">input_s3_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;s3a://spark-bucket/Shakespear.txt&quot;</span>
<span class="n">output_s3_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;s3a://spark-bucket/Shakespear_Result&quot;</span>

<span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.s3a.access.key&quot;</span><span class="p">,</span> <span class="s2">&quot;GNM6UQ2A5H2JUNQ96APA&quot;</span><span class="p">)</span> <span class="c1"># 之前的账号信息</span>
<span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.s3a.secret.key&quot;</span><span class="p">,</span> <span class="s2">&quot;9pDusNV9x1ZDan84HNLYdx6COBrhpr5k1uuimJkR&quot;</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.s3a.endpoint&quot;</span><span class="p">,</span> <span class="s2">&quot;http://192.168.184.164:7480&quot;</span><span class="p">)</span> <span class="c1"># ceph rgw</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">input_s3_path</span><span class="p">)</span>

<span class="n">words</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span>

<span class="n">counts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">counts</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="n">output_s3_path</span><span class="p">)</span>
</pre></div>
</div>
<p>结果如下图所示，可以看到利用S3能够成功读取:</p>
<img alt="_images/spark_rp1i.png" src="_images/spark_rp1i.png" />
<p>并且已经将数据存储到Ceph当中，到此S3 gateway endpoint方案成功运行：</p>
<img alt="_images/spark_rp2i.png" src="_images/spark_rp2i.png" />
</div></blockquote>
</div>
<div class="section" id="c-hadoop-cephfs">
<h2>c. 使用hadoop-cephfs<a class="headerlink" href="#c-hadoop-cephfs" title="永久链接至标题">¶</a></h2>
<div class="section" id="spark-hdfs">
<h3>1). Spark + HDFS<a class="headerlink" href="#spark-hdfs" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>spark最初就是从hdfs中存储数据的，同样先测试spark读取hdfs:</p>
<div class="highlight-shell"><div class="highlight"><pre><span></span>/spark/bin/spark-submit --master spark://172.18.0.2:7077 wordcount_hdfs.py
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># wordcount_hdfs.py</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>


<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
        <span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;WordsCount&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="n">input_hdfs_path</span> <span class="o">=</span> <span class="s1">&#39;hdfs://mynamenode/data/Shakespear.txt&#39;</span>
<span class="n">output_hdfs_path</span> <span class="o">=</span> <span class="s1">&#39;hdfs://mynamenode/data/Shakespear_Result&#39;</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">input_hdfs_path</span><span class="p">)</span>

<span class="n">words</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span>

<span class="n">counts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">counts</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="n">output_hdfs_path</span><span class="p">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p>运行结果如下图所示，结果也存入hdfs中:</p>
<img alt="_images/spark_rp4i.png" src="_images/spark_rp4i.png" />
</div></blockquote>
</div>
<div class="section" id="hadoop-cephfs">
<h3>2). Hadoop + CephFS<a class="headerlink" href="#hadoop-cephfs" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Hadoop结合CephFS就是依然使用的是HDFS分布式文件系统，但是将数据存储在CephFS里面。由于hadoop集群的各个节点都是容器,
以及较新的的Ceph和hadoop版本，实际上本次Hadoop + CephFS是失败的（配置好之后数据依然存在hdfs里面而不是cephfs)。另外，S3对象存储相比，Hadoop + CephFS方案的数据访问速率是比较慢的，所以在实际部署时仍待商榷，记录Hadoop + CephFS只是做一个参考，<a class="reference external" href="http://www.178pt.com/175.html">参考文章</a> 。</p>
</div>
<p>Hadoop + CephFS需要各个节点cephfs-java，libcephfs-jni-devel等相关插件的支持，插件可以通过配置ceph源安装或者直接下载编译（本次实验）。同时还需要<a class="reference external" href="http://download.ceph.com/tarballs/hadoop-cephfs.jar">hadoop-cephfs.jar</a>。</p>
<ol class="arabic">
<li><p class="first">各节点安装cephfs-java等插件插件并创建软连接：</p>
<blockquote>
<div><div class="highlight-shell"><div class="highlight"><pre><span></span>docker <span class="nb">exec</span> -it mynamenode /bin/bash
<span class="nb">cd</span> /opt/hadoop-2.7.4/lib/native/
ln -s /usr/lib64/libcephfs_jni.so .
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">将hadoop-cephfs.jar添加到/usr/share/java目录：</p>
<blockquote>
<div><div class="highlight-shell"><div class="highlight"><pre><span></span>docker <span class="nb">exec</span> -it mynamenode /bin/bash
<span class="nb">cd</span> ~
cp hadoop-cephfs.jar /usr/share/java/
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">修改Hadoop的运行环境配置文件/etc/hadoop/hadoop-env.sh，如图添加：</p>
</li>
</ol>
<img alt="_images/spark_rp5i.png" src="_images/spark_rp5i.png" />
<ol class="arabic" start="4">
<li><p class="first">将ceph配置文件拷贝到各节点：</p>
<blockquote>
<div><div class="highlight-shell"><div class="highlight"><pre><span></span>docker cp /root/ceph/etc/ceph.conf mynamenode:/root/ceph/
docker cp /root/ceph/etc/ceph.conf mydatanode1:/root/ceph/
docker cp /root/ceph/etc/ceph.conf mydatanode2:/root/ceph/

docker cp /root/ceph/etc/ceph.client.admin.keyring mynamenode:/root/ceph/
docker cp /root/ceph/etc/ceph.client.admin.keyring mydatanode1:/root/ceph/
docker cp /root/ceph/etc/ceph.client.admin.keyring mydatanode2:/root/ceph/
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">修改hadoop配置/etc/hadoop/core-site.xml</p>
<blockquote>
<div><div class="highlight-xml"><div class="highlight"><pre><span></span><span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="cp">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span>
<span class="c">&lt;!--</span>
<span class="c">Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c">you may not use this file except in compliance with the License.</span>
<span class="c">You may obtain a copy of the License at</span>

<span class="c">    http://www.apache.org/licenses/LICENSE-2.0</span>

<span class="c">Unless required by applicable law or agreed to in writing, software</span>
<span class="c">distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c">See the License for the specific language governing permissions and</span>
<span class="c">limitations under the License. See accompanying LICENSE file.</span>
<span class="c">--&gt;</span>

<span class="c">&lt;!-- Put site-specific property overrides in this file. --&gt;</span>

<span class="c">&lt;!-- file system properties --&gt;</span>
<span class="nt">&lt;configuration&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>ceph://192.168.184.164/<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>fs.ceph.impl<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>org.apache.hadoop.fs.ceph.CephFileSystem<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>ceph.conf.file<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>/root/ceph/ceph.conf<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>ceph.data.pools<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>cephfs_data<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>ceph.mon.address<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>192.168.184.164:6789<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;description&gt;</span>This is the primary monitor node IP address in our installation.<span class="nt">&lt;/description&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>ceph.auth.id<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>admin<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>ceph.auth.keyring<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>/root/ceph/ceph.client.admin.keyring<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">重启Hadoop集群，正常情况下应该可以通过HDFS将文件存储至CephFS中。</p>
<blockquote>
<div><div class="highlight-shell"><div class="highlight"><pre><span></span>docker restart mynamenode mydatanode1 mydatanode2
docker <span class="nb">exec</span> -it mynamenode /bin/bash

<span class="c1"># 上传测试</span>
hdfs dfs -put test.txt /data/
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, lyfan

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>